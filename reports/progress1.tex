% Bryan Dixon
% Brendan Kelly
% Mark Lewis-Prazen
% Andy Sayler
% University of Colorado
% Distributed Systems
% Spring 2012

\documentclass[11pt]{article}

\usepackage[text={6.5in, 9in}, centering]{geometry}
\usepackage{graphicx}
\usepackage{url}
\usepackage{listings}
\usepackage{hyperref}

\bibliographystyle{plain}

\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\newenvironment{packed_enum}{
\begin{enumerate}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{enumerate}}

\newenvironment{packed_item}{
\begin{itemize}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}}

\begin{document}

\title{
  Distributed File Storage Systems\\
  Project Report 1
}
\author{
  Bryan Dixon \and Brendan Kelly \and Mark Lewis-Prazen \and Andy Sayler\\
  \and University of Colorado\\
  \texttt{first.last@colorado.edu}
}
\date{\today}

\maketitle

\begin{abstract}
Modern applications routinely process
terabytes, and even petabytes, of data. This capability has largely been
achieved by distributed processing architectures and methodologies.
Companies such as Google, Amazon and Yahoo have embraced such
technologies over the past decade and made them a
central part of their business models. 
Computer science researchers and professionals must become well versed
in large scale data processing and alternative storage architectures
and service strategies in order to make technological and
architectural choices these areas. While storage technology has
evolved significantly over the past five years, a deep understanding
of both technical and business storage issues has lagged behind.

The intent of our project is to install and configure several current 
open source distributed storage file systems in a test environment 
with the intent of running a battery of tests to evaluate their 
performance across a series of accepted storage system metrics. In 
doing so we hope to gain a better understanding for the tradeoffs 
involved in the architectural design of such systems.


\end{abstract}

\newpage

\section{Problem Definition}
Along with the infrastructure and network based applications, data
storage is recognized as one of the major components of information
technology systems.  Functionally, storage typically services a wide
range of requirements, spanning the spectrum from caching to archival
needs. Combining networking and storage has created a platform with
numerous possibilities allowing Distributed Storage Systems (DSS) to
adopt roles which can vary considerably and may fall well beyond
traditional data storage needs. Most DSSs as we understand them today
were in their inception known as Distributed File Systems (DFS).
Networking infrastructure and distributed computing share a close
relationship. Advances in networking are typically closely followed by
new distributed storage systems, which are designed to better utilize
the enhanced capabilities of the network. Currently, the next
generation Internet based systems are encountering numerous
challenges, among them longer delays, unreliability, unpredictability
and the potential for malicious behavior, all because of the need to
operate in a public shared environment. To address this new set of
challenges, innovative new storage architectures and algorithms are
being suggested and developed, offering a myriad of improvements in
areas such as security, consistency and routing. As storage systems
continue to evolve, they naturally increase in complexity. Hence, the
requisite expertise required to implement and operate them also
increases dramatically.

Industry storage practices have evolved considerably over the past
decade. The amount of data being actively managed continues to expand
at around 20\% per year, with many firms dealing with growth rates
exceeding 50\%. At these levels, most data centers will double storage
capacity every two to three years. Meanwhile, IT budgets are generally
in decline. Why isn't industry-wide chaos rampant? In a word, this is
due to consolidation efforts. Consolidation is being aided by
continued evolution of high-density magnetic media as well as bigger,
faster, and less expensive solid-state drives. Virtualization is also
helping by easing management of aggregated storage pools that are
using available capacity more efficiently. Optimization technologies
like data reduction, thin provisioning, and automatic tiering are also
providing significant benefits.

What has become increasingly apparent in production environments is
that the storage decision is no longer relegated to a back office
bureaucracy which manages an organization’s storage needs en masse.
Rather, as organizations increasingly decentralize and decouple
decision-making regarding many applications either in the development
or maintenance cycle, storage decisions are being made on a more ad
hoc basis. Also, as more organizations consider or adopt partial or
full cloud or virtualization storage strategies, storage decisions
have become more application-centric. Since full migrations to the
cloud or to virtual storage models are generally infeasible in the
short term, except in the case of smaller organizations, more firms
are increasingly managing a tiered storage model. Recent published
research by International Data Corporation (IDC) and others
demonstrate that storage utilization rates achieved by most
U.S. companies are typically 40\% or lower. Hence there appears to be
considerable room for improvement in both storage management and
implementation practices across the industry.

The above noted disruptive forces in the storage industry are creating
a unique set of opportunities and challenges. New firms are appearing
that offer storage solutions and services bundled in ways which were
virtually unheard of five to seven years ago. At the same time
organizations and IT managements under increasing pressure to reduce
storage costs, increase storage utilization rates and provide both a
secure and transparent accessibility to their data users regardless of
the ultimate location of a particular data set. Our project is
focussed on providing methods and techniques to allow the computer
science professionalto better understand and navigate this new and
complex storage landscape and to discover the choices and tradeoffs
that one must consider in designing and implementing a sensible
organization-wide storage solution.

\section{Introduction}
Distributed storage systems have evolved from providing a means to
store data remotely, to offering innovative services like publishing,
federation, anonymity and archival. To make this possible networks
have also evolved, generally as a leading indicator of storage
evolution. With network infrastructure undergoing another quantum leap
recently, and outpacing the bandwidth capability of processors and
hard-drives, this provides a platform for future distributed storage
systems to offer more services yet again.

The emergence of Cloud Computing, one of the new variables in the
storage arena, requires organizations to move from server-attached
storage to distributed storage. Along with variant advantages, the
distributed storage also poses new challenges in creating both a
secure and reliable data storage and access facility over insecure or
unreliable service providers.  The security of data stored in the
cloud is one of the challenges to be addressed before the
pay-as-you-go Storage as a Service (STaaS) model can become widespread
in the industry. In the enterprise, STaaS vendors are now targeting
secondary storage applications by promoting STaaS as a convenient way
to manage backups. The key advantage to STaaS in the enterprise is in
cost savings in personnel, in hardware and in physical storage
space. For instance, instead of maintaining a large tape library and
arranging to vault (store) tapes offsite, a network administrator that
used STaaS for backups could specify what data was to be relegated to
outsourced storage and storage service providers would manage the data
from that point. If the company's data ever became corrupt or was
lost, the network administrator could contact the STaaS provider and
request a copy of the data. For these reasons, STaaS is generally seen
as a good alternative for a small or mid-sized business that lacks the
capital budget and/or technical personnel to implement and maintain
their own storage infrastructure. STaaS is also being promoted as a
way for all businesses to mitigate risks in disaster recovery, provide
long-term retention for records and enhance both business continuity
and availability. Obviously, for larger businesses, the movement to a
STaaS model or a partial such model is a longer term proposition as
storage equipment reaches the end of its useful life and storage
agreements expire. The existence of long term contracts and sizable
amortization schedules often make movement to more flexible business
storage strategies less likely in the short term. Hence many large
organizations are maintaining tiered stage models; at least in the
short term planning horizon.

Other trends are equally disruptive and compelling. Full provisioning,
the practice of provisioning all the capacity of an external disk to a
given app has been accepted practice in industry circles for some
time. This practice ensures that a given application has sufficient
storage to meet projected growth potential. However, though it
typically results in poor utilization rates as noted above. So
essentially, a surplus of storage capacity is being acquired, which
generally translates into more space and cooling costs in addition to
higher overhead costs since unused capacity still needs to be
monitored and managed. When applications reach capacity limits and
re-provisioning is necessary, the costs increase even more. In adding
additional capacity, complex management tasks can be
involved. Finally, when an app is taken offline to re-provision
capacity, it is then unable to serve business needs and can lead to
revenue loss. Thin provisioning has been offered as a solution to
address many of the issues noted above. By automatically allocating
system capacity to applications as needed, this technology can result
in storage utilizations levels of up to 90\%, while simultaneously
reducing power consumption. This technique allows users to allocate a
large amount of virtual capacity for an application, regardless of the
physical capacity actually available. This on-demand method not only
optimizes storage utilization, but also greatly simplifies capacity
planning and management. In order to help users easily monitor
capacity utilization, storage systems automatically notify when the
capacity utilization is reaching some pre-defined limit. A decision to
expand capacity can be done seamlessly.

Under traditional provisioning practices, it is difficult to move data
across logical partitions in a storage architecture. When thin
provisioning is used, storage capacity from different logical
partitions can be combined, enabling storage to be dynamically
allocated. Conversely, this means that the storage controller can move
data dynamically across logical partitions based on how resources are
designed to function. Also, thin provisioning allows other advances in
storage design, including automated storage tiering. Storage tiering
involves grouping data into different categories and assigning these
categories to different types of storage media in order to optimize
storage utilization. Automated tiering ensures applications have
access to the performance levels they need so they can be properly
paired up. For example, high performance applications can be assigned
to high performance storage tiers featuring drives such as SSDs or
SAS, while applications requiring less performance can be assigned to
lower tiers featuring low performance drives such as SATA. This
ensures that storage resources are not squandered and that
applications function effectively. Finally, the new technology helps
automatically migrate data based on usage patterns. So, if data in
higher storage tiers has not been used for an extended period of time,
it is demoted to lower storage tiers. Conversely, if data in lower
tiers is frequently accessed, it is promoted upward. Such techniques
can greatly improve storage efficiency.

Obviously, the RDBMSs of today aren't going away, certainly not in the
short term. However, storage requirements for the new generation of
applications are dramatically different. The Semantic Web / Web 3.0 is
going to be full of semi-structured data on an even larger scale, so
it's prudent for applications to take advantage of the upcoming
technologies as soon as possible. Future killer applications and
appliances will have to connect to the cloud and hence will be written
with distributed storage in mind, whether the applications run on the
desktop or on the web. Undeniably, the design of distributed storage
poses many challenges - scalability, hardware requirements, query
model, failure handling, data consistency, durability, reliability,
efficiency, etc. The landscape of storage architectures/software we
describe in this paper are helping address many of the concerns raised
with respect to current shortfalls in distributed storage architecture
and methodology. But, clearly, the future of data and storage is a
virtyual model. Our project explores many of these issues as well as
the available options and tradeoffs inherent in these choices which
are currently shaping Distributed Storage Systems. In the next section
we examine the current literature on distributed storage.

\section{Related Work}
The literature on distributed storage dates back to the
late 1980’s. Early works typically provide perspective as well as
insight into issues related to building toward the DSS of today. More
recent works focus on more cutting edge research generally in areas
like peer-to-peer and data grid systems. They also focus on some of
the more important issues occurring like routing, consistency,
security, auto management and federation.

A number of studies over the past decade have designed and evaluated
large- scale, peer-to-peer distributed storage systems. Redundancy
management strategies for such systems have also been well evaluated
in prior works.  Among these, several compared replication with
erasure codes in the bandwidth -reliability tradeoff space. The
analysis of Weatherspoon and Kubiatowicz demonstrated that erasure
codes could reduce bandwidth use by an order of magnitude as compared
with replication. The authors showed that in high- turnover scenarios
erasure codes provide large storage benefits but the bandwidth cost is
too high to be practical for a P2P distributed storage system. In
low-churn environments, the reduction in bandwidth is fairly small. In
moderate-churn environments, there is some benefit, but this is
generally negated by the added architectural complexity that erasure
codes introduce into the overall architecture. Other types of systems
with publish/share features include NFS, Coda, xFS and Ivy
\cite{Muthitacharoen:2002}. Unlike pure archival systems where the
storage service aims to be persistent, the publish/share category is
somewhat volatile as the main objective here is to provide a
capability to share or publish files. The volatility of storage is
usually dependent on the popularity of the file. True DSSs in the
performance category are typically used by applications which require
a high level of performance. The large proportion of systems in this
category would be classified as Parallel File Systems (PFSs).  PFSs
typically are found within a computer cluster, satisfying storage
requirements of large I/O-intensive parallel applications.  Systems
which fall into this category include PVFS \cite{Carns:2002}, Lustre
\cite{Braam:LustreWhite} and GPFS \cite{Schmuck:2002}.

Finally, the custom category has been created for storage systems that
have come into vogue in the past few years, and typically possess a
unique set of functional requirements generally customized for a
particular environment.  Systems in this category may fit into a
combination of the above system categories and exhibit unique
behavior. Google File System (GFS) \cite{Ghemawat:2003p1274} and
OceanStore \cite{Kubiatowicz:2000,Rhea:2003}, are such
systems. GFS was designed and built with a particular functional
purpose which is reflected in that design. Similarly, OceanStore
presents itself as a global storage utility, providing many interfaces
including a general purpose file system. To ensure scalability and
robustness in the event a failure occurs, OceanStore employs P2P
mechanisms to distribute and archive data. Likewise, Freeloader
\cite{Vazhkudai:2005} combines storage scavenging and striping,
achieving good parallel bandwidth on shared resources. The collection
of features available from Freeloader, OceanStore and GFS all exhibit
unique qualities for a specific set of functional and technical
requirements.

In the subsections below we summarize the recent literature with
respect to some individual storage system architectures. The
architecture is important as it determines the application’s
operational boundaries, ultimately driving both behavior and
functionality. As well as functional qualities, the architecture also
has an impact on the means a system may use to achieve consistency,
routing and security. One can from the systems discussed below that
the evolution of architectures adopted by DSSs have gradually moved
away from centralized to more decentralized approaches, primarily as a
result of the need for scalability as well as the challenges
encountered in operating across a dynamic global network.


\subsection{Tahoe File System}

The Tahoe-LAFS (Tahoe Least-Authority Filesystem) is a secure,
distributed filesystem designed by Zooko Wolcox-O'Hearn and Brain
Warner of allmydata.com. Tahoe-LAFS (henceforth referred to as
'Tahoe') is designed around the Principle of Least Authority, and aims
to allow one to deploy a trusted distributed filesystem using
untrusted, or even actively malicious, nodes. Tahoe is also designed
to be fault-tolerant, and to run on commodity hardware where failures
may occur frequently. Tahoe uses Reed-Solomon erasure coding to obtain
redundancy across nodes. It uses convergent encryption in conjunction
with a capability-based access control model, to obtain and maintain
security \cite{WilcoxOHearn:2008p1275}.

Tahoe presents a web-API to users of the system that can be used to
administrate the system, as well as to read, write, or verify files
and directories on the system. This web-based API makes Tahoe suitable
for use as the backend for various services and applications in
domains such as backup and cloud storage. Tahoe is still actively
maintained and is Free Software under the GPL and TGPL. It served as
the backend for allmydata.com until allmydata.com went out of
business. It is currently deployed for personal use be a number of
individuals, and serves as the storage backend for several other
systems \cite{tahoe-lafs.org}.

\subsection{Ceph}
Ceph is an object-based parallel file system with a scalable metadata
implementation with data replication to allow for data recovery and
recovery from hardware failures
\cite{Weil:2012p1035,Weil:2012p1010,Weil:2006p1273}.
Ceph is similar to that of many other
distributed file systems in it has metadata servers that a client uses
to determine where the file objects live. The key designs for Ceph are
providing the ability to scale easily which make it great for use in
cloud storage or in cases where hardware for the system may be added
or removed. Unlike Tahoe, Ceph lacks strong encryption an a built-in
option. It is
possible that encryption could be applied by the client prior to
storing their files to add a level of security over the default POSIX
access control model.

\subsection{Google File System}
The Google File System is designed for massive amounts of data and is 
optimized for scalability. It's relevance to this project is in some 
of the techniques it uses to approach issues arisen from the use of 
commodity hardware and software.

\subsection{Dynamo Data Store}
Amazon's Dynamo is a highly scalable distributed data store custom
built for Amazon’s business needs. Dynamo is a completely
decentralized system with minimal need for manual
administration. Storage nodes can be added and removed from Dynamo
without requiring any manual partitioning or redistribution. The
system is used to manage a set of services with high reliability
requirements and tradeoffs between availability, consistency,
cost-effectiveness and performance. Dynamo uses a collection of well
known techniques to achieve scalability and availability. First, data
is partitioned and replicated using consistent hashing. The
consistency among replicas during updates is maintained by a
decentralized replica synchronization protocol.

Dynamo is designed to be an eventually consistent data store in the
sense that all updates reach all replicas ultimately. The challenge
with such an approach is that it can lead to conflicting changes that
require eventual resolution. This process of such a resolution event
introduces two subproblems: when to resolve them and who assumes the
responsibility for final resolution. Specific business requirements
result in Dynamo pushing the complexity of conflict resolution to the
reads in order to ensure that writes are never rejected.

Other important design principles demonstrated in the Dynamo system
include: (1) incremental scalability which allows the system to scale
out one storage nodes one at a time, with minimal impact on both
operators of the system and the system itself, (2) symmetry whereby
every node in Dynamo has the same set of responsibilities as its peers
with no node(s) taking on special responsibilities and thereby
simplifying both provisioning and maintenance, (3) decentralization
which leads to more scalable and available system, and, (4)
heterogeneity which allows the system to make the work distribution
more proportional to the capabilities of each server.


\section{Design}

Cloud storage services are examples of a fully distributed storage 
facility. These services have a number of attractive features, such 
as pay per usage and guarantees that data is safely stored via 
redundant methods. Typically, though, they don't provide high speed 
access and support for a wide range of different applications. This 
is also true for storage solutions aimed at grid computing; in a grid 
infrastructure, storage facilities are focused on supporting computation 
at grid nodes, not on providing high-speed data access to applications at 
the periphery  of the network.  Also, when data is stored in a distributed 
system, it is advantageous to position data where it is most often used, 
and migrate less important or less frequently accessed data to areas with 
cheap capacity. Such a system is likely to have different storage level 
(or tiers) that form a storage hierarchy. 

Examining systems that offer general purpose functionality is useful. We 
identified the following high level requirements as relevant for the 
distributed storage system. They are typically qualitative requirements, 
as these are sufficient to survey existing products and sufficient to make 
initial product evaluations. We are aware that a WAN distributed storage 
system may not be able to offer features in any combination, or that the 
CAP theorem applies which states that only two out of the three properties 
data consistency, system availability and partition tolerance can be 
supported by a distributed system. Ideally, though, products should be able 
to explicitly balance these requirements, such that in a future system a 
property trade-off is easy to configure and evaluate.

\begin{itemize}
\item Scalable The system must be scalable in terms of capacity, performance 
and in terms of concurrent access. Hence, it should be easy to expand the 
total amount of storage without degrading performance and concurrency. Also, 
it should be relatively simple to configure the system such that a large 
number of users concurrently can obtain access also concurrently to 
individual storage objects without performance suffering markedly.

\item High Availability The system should have high-availability functionality 
that keeps data available to applications and clients, even in the event of 
software or hardware errors. This implies that the system is capable of 
replicating data at multiple locations. It also implies that capacity and 
storage locations may be added or removed while the system is operating.
\item Durability The system should support the storage of data in a durable 
manner, so that if hardware or software failures occur, no data is lost. 
Durability functionality includes maintenance to make sure that a minimum 
number of replicas are available. 

\item Performance at Traditional SAN/NAS Level The system should be able to 
support a level of performance (bandwidth \& latency) comparable to a 
traditional non-distributed environment. Being a general purpose system, 
it is clear that the supported performance for specific applications will 
not match the performance of dedicated storage solution. However, by 
offering different kinds of storage tiers within the distributed system, 
it should be possible to offer different levels of performance. 

\item Dynamic Operation The level of availability, durability and performance 
should be configurable per application. This allows the system to always 
run at the highest supported level of functionality, thereby reducing costs. 
Moreover, it allows user, application developers and system administrators 
to balance cost versus features. The system should also be self-configurable 
and self-tunable, in the sense that it changes parameters to optimize its 
own operation. The system should support the movement of  data between 
different kinds of storage technologies thus providing tiered’ 
functionality so that data objects that are accessed frequently are 
stored on the disks with highest performance and those that are infrequent 
accessed are stored on slower disks. 

\item Cost Effective It should be possible to build, configure, run, and 
maintain the system in a cost-effective manner. The system should work with 
commodity hardware. Configuration of the system, as well as the maintenance 
should be easy and straightforward. 

\item Interfaces The system should offer generic interfaces to applications 
and clients. Preferably it supports the POSIX file system interface; this 
implies that a wide range of different applications can be supported.  
Alternatively, the system could support a block device interface, because 
such an interface can be used to implement arbitrary file systems.

\item Protocols Based on Open Standards The system should be built using 
protocols based on open standards where possible. This reduce reduces the 
possibility of vendor lock-in and improves extensibility. In many respects 
this also relates to cost effectiveness since, in the long run it should 
typically be more economical to maintain such a system than a proprietary 
based alternative.

\item Multi-Party Access The system should support access by multiple, 
geographically dispersed parties at the same time. This enables 
collaboration between these parties over long distances on the same data.

\end{itemize}

In this project we wanted to understand competing products and to 
implement two to three distributed storage systems with qualities as 
described above in a networked environment and then execute a number of 
tests to understand the performance characteristics of the individual 
systems and their inherent trade-offs. For our systems we chose Tahoe, 
Ceph and Lustre respectively, as they represent some of the relatively 
new storage alternatives available and, yet, have divergent enough 
features to provide an interesting dichotomy for our testing and 
benchmarking purposes. In this section we provide a brief discussion 
of each of the respective systems, their distinct architectural traits 
and salient features and some comparative and contrasting of each of 
the systems.  

\subsection{Tahoe}
Tahoe is an open source distributed file system that supports storing 
files into a network of peers. All data in a Tahoe configuration is 
written in an encrypted manner to the storage nodes. It uses erasure 
coding to spread the pieces of a file over a number of nodes in a 
redundant way and to improve data durability. It was originally 
developed with funding from Allmydata, a company that provides Web 
backup services. The company had some highly ambitious plans for 
distributed storage. It initially offered a service through which 
individual consumers could get cheap storage capacity on the 
distributed grid in exchange for volunteering to let the grid use 
some of their own local storage. The idea was that every user would 
be able to get the benefits of distributed off-site backups by sharing 
a portion of their local drive space with the rest of the network. The 
company eventually moved away from that strategy and now self-hosts all 
of their backup storage. The Tahoe source code, which is made available 
under the terms of GNU's General Public License (GPL), can be used to 
build distributed storage grids that function in much the same manner 
as the company’s original concept.

\subsubsection{Tahoe Access Control}
Tahoe uses a capability-based access control model to manage access to 
files and directories. Each file/directory in a Tahoe grid is identified 
by a capability which is a short and unique string that designates the 
file/directory and gives the user the authority to perform a specific 
set of actions (reading or writing) on that file/directory. This access 
scheme is known as "capability as keys" or "cryptographic capabilities". 
The elements of the access model are files and directories and the 
operations supported by the access model are sharing and revocation and 
are described as follows: 

File could be either Immutable which is created once in the grid and read 
for multiple times or Mutable "slot" "container" which has a read and 
write access. Immutable file has two capabilities, the read-cap for 
reading the file and the verify-cap for checking the file and allows 
untrusted nodes to preform checking/repairing on the file without having 
the ability to read the content of the file. 

Mutable file has three capabilities, the read-only cap for retrieving 
last version of the file, the read-write cap for reading the file like 
read-only cap and also to write a new version of the file , and the 
verify cap. read-only cap could be derived from read-write cap and a 
verify cap could be derived from a read-only cap. Directory is a mutable 
file containing a list of entries, each entry defines a child by its name, 
metadata, read-cap and read-write-cap. Directories have the property of 
transitive read-only which enable users who have read-write access to a 
directory to have a read-write access to its children, but users who have 
read-only access to a directory will have a read-only access to its 
children. Sharing is done simply by revealing the capability of the shared 
file/directory, transitive read-only property will limit the access to 
sub-directories according to the type of the revealed capability. 
Revocation is done by deep-copying  of the shared folder to another 
location and reveal the cap of the new location to the authorized users 
only. The unauthorized users still will continue to have access to the 
old directory but cannot see the new changes.

\subsubsection{Tahoe Architecture}
Tahoe's underlying architecture is similar to that of a peer-to-peer 
network. Files are distributed across multiple nodes in a manner that 
allows data integrity to be maintained in the event that individual nodes 
are compromised or fail. The system uses AES encryption to protect file 
contents from tampering and scrutiny. Tahoe can be used to establish a 
relatively fault-tolerant storage pool that spans a number of conventional 
computers over a local network or, conversely, over the Internet. This 
approach to cloud storage is often described as "crowd" storage.

Tahoe could be thought of quite simply as collection of three layers: 

\begin{enumerate}
\item Key-Value store : in which each key "capability" identifies a 
file/directory "value" on the grid. It provides PUT and GET operations. 

\subitem key = PUT(data) 

\subitem data = GET(key)

\item Decentralized File System Layer:  The key-value store provides the 
mapping from URI to data, decentralized file system layer turn this into 
a graph of directories and files where files are leaves. It needs to keep 
track of directory nodes to maintain a graph of files and directories. 

\item Application Layer:  Application specific features that use the 
underlying layers for example Allmydata.com used it for a backup service: 
the application periodically copies files from the local disk onto the 
decentralized file system. When a file is deployed to Tahoe, it is 
encrypted and split into pieces that are spread out across ten separate 
nodes. Using a variation of Reed-Solomon error correction, it can 
reconstruct a file using only three of the original ten nodes. This helps 
to ensure data integrity when some nodes are unavailable. This is a bit 
similar to how RAID storage works. Tahoe uses a library called zfec that
provides an efficient implementation of the error correction code and 
exposes it through a Python API.

\end{enumerate}

Although Tahoe is a distributed file system, it is not entirely 
decentralized in the strictest sense of the word. It needs a central node, 
called an Introducer, which is responsible for getting new nodes connected 
to existing nodes on the grid. Tahoe is designed to minimize its dependency 
on the Introducer, but it's still a central point of failure. If the 
Introducer fails for any reason, existing nodes will still be able to 
communicate with each other and propagate data but the grid won't be able 
to connect new nodes. The developers hope to address this limitation in a 
future version.

Tahoe is currently being used in a number of different ways. A common 
configuration that is documented at the project's wiki is described as a 
"friendnet", a group of roughly ten nodes that are connected over the 
Internet and provide shared secure storage capacity with optional 
filesharing. Another potential usage scenario is installing Tahoe on 
individual workstations on an office network and using their excess disk 
capacity as a storage pool. The Tahoe wiki describes that kind of setup 
as a "hivecache".

The Tahoe source code is primarily written in Python with the Twisted 
framework. The code base is highly portable and can run on Windows, 
Mac OS X, Linux, Solaris, and several flavors of BSD and is now part 
of the Ubuntu Linux distribution. It runs entirely in user space and 
doesn't require any kernel modules or other low-level components. It 
works well on regular commodity hardware and doesn't have any particularly 
special requirements. Installation instructions are available at the 
project's web site.

Tahoe provides very little control over on which nodes data is stored, 
which makes it unsuitable for tiered functionality applications. 
Furthermore, Tahoe assumes a flat local network environment and 
therefore is not suitable to run in a WAN. 


\subsection{Ceph}
Ceph is a distributed file system originally designed by the Storage 
Systems Research Center at the University of California, Santa Cruz. 
It is developed as an open source project, with code under the GNU LGPL 
license, and, since May 2010, has its client integrated in the Linux 
kernel. The objectives of Ceph are to provide a fully distributed file 
system without a single point of failure, with a POSIX-style interface. 
It claims high I/O performance and a high level of scalability. The 
file system has three main components: the client, each instance of 
which exposes a near-POSIX file system interface to a host or process; 
a cluster of object storage devices, which collectively stores all 
data and metadata; and a metadata server cluster, which manages the 
namespace (file names and directories) while also coordinating 
security, consistency and coherence.

\subsubsection{Ceph Architecture}
Ceph's architecture consists of two main components: an object storage 
layer, and a distributed file system that is constructed on top of 
this object store. The object store provides a generic, scalable 
storage platform with support for snapshots and distributed computation. 
This storage backend is used to provide a simple network block device  
with thin provisioning and snapshots, or an S3 or Swift compatible RESTful 
object storage interface. It also forms the basis for a distributed file 
system, managed by a distributed metadata server cluster, which similarly 
provides advanced features like per-directory granularity snapshots, and a 
recursive accounting feature that provides a convenient view of how much 
data is stored beneath any directory in the system.

The primary goals of the architecture are scalability to hundreds of 
petabytes and beyond, performance, and reliability. Scalability is 
considered in a variety of dimensions, including the overall storage 
capacity and throughput of the system, and performance in terms of 
individual clients, directories, or files. Ceph directly addresses the 
issue of scalability while simultaneously achieving high performance, 
reliability and availability through three fundamental design features: 
decoupled data and metadata, dynamic distributed metadata management, 
and reliable autonomic distributed object storage.

Ceph is based on an object storage paradigm, where file data is stored 
by object storage devices (OSDs) and metadata is stored by metadata 
servers (MDSs). Contrary to some distributed file systems relying on 
dumb’ OSDs, the Ceph OSDs have responsibilities for data migration, 
replication and failure handling and communicate between each other. 
Metadata management is completely distributed, using a cluster of MDSs 
to handle metadata request from clients. The operation is adapted 
dynamically based on the workload generated by the clients (e.g., moving 
and replicating metadata depending on how often a file is accessed).
An MDS does not keep track of which OSDs store the data for a particular 
file. Instead, Ceph uses a special function called CRUSH to determine the 
location of objects on storage nodes: it first maps an object to a 
placement group, and then calculates which OSDs belong to that placement 
group. While doing so, it takes care of the replication of file data on 
different OSDs. CRUSH automatically takes into account that the set of 
storage nodes is dynamic over time. To clients, the data within a Ceph 
configuration (potentially consisting of thousands of OSDs) is presented 
as a single logical object store called RADOS. Replication of data is 
organized by writing to the first OSD in the placement group, after which 
this OSD replicates the data to others. The client receives an ack when 
all data has reached the buffer caches on all OSDs, and receives a commit 
when the data has been safely stored on all involved OSDs. RADOS has 
mechanisms for failure detection and automatic re-replication. Furthermore, 
Ceph implements a mechanism for recovery in case of system outages or 
large configuration changes.

\subsubsection{Ceph Design Considerations}
A Ceph cluster consists of a few different pieces: the monitors (mons), 
storage nodes (osds), and metadata servers (mds). One each is the minimum 
requirement , but with one the system will have no  redundancy.
The storage nodes will store the actual data on the disk. The minimum 
requirement here is two if the system is to have any redundancy across 
nodes. Each storage node is essentially an instance of the Ceph-OSD, 
providing access to a local disk or set of disks. The simplest option is 
to use one Ceph-OSD daemon per hardware node and then pool the disks. 
Ceph generally is found to work best when using BTRFS, but other file 
systems like ext3 and ext4 represent other viable choices. Btrfs provides 
checksumming and the ability to pool disks together without RAID, can 
internally handle data redundancy and facilitates Ceph journaling. 
The Ceph Metadata Server functions as a distributed, cache of file system 
metadata. It does not store data locally, rather all metadata is stored on 
disk via the storage nodes. MDS daemons can be added into the cluster as 
needed. Typically guidelines suggest starting with 1 or 2, then add more 
as necessary.  The 'max mds' parameter controls how many cmds instances 
are active. Additional running instances are put in standby mode. The 
Ceph Monitor daemons administer central cluster management, configuration, 
and state. Data is stored in a directory on the local host file system. 
They must be deployed in odd-number sets. One daemon is acceptable; three 
is ideal for most use cases. More may be required for extremely large 
clusters. Other general cluster suggestions are: (1) For a few nodes,  
put Ceph-MON, Ceph-MDS, and Ceph-OSD on the same node(s). (2) For more 
nodes,  put Ceph-MDS and Ceph-MON together, and Ceph-OSD on the disk nodes. 
(3) For even more nodes, separate Ceph-MDS, Ceph-MON, and Ceph-OSD all 
onto dedicated machines.

A major drawback is the immaturity of Ceph as a platform: by all acccounts 
in the published literature, it has not been widely used in a production 
environment, and the Ceph documentation also explicitly warns of the beta 
state of the code. Another issue is the uncertainty about the operation 
of Ceph in a WAN environment. The placement of the file data at OSDs does 
not take into account that links between storage nodes have variable 
quality (bandwidth, latency). Additionally, the operation of the mechanisms 
for automatic adaptation (adjustment of placement group to OSD mapping, 
and failure detection) may be non-optimal or worse for a WAN Ceph 
configuration.

\subsection{Lustre}
Lustre is a massively parallel distributed file system running on Linux 
and used at many high-performance computing (HPC) centers worldwide. 
Originally developed at CMU in 1999, it is now owned by Oracle and the 
software is available as open source under a GNU GPL license. The Lustre 
architecture defines different kinds of roles and nodes, following an object 
storage paradigm where metadata is separated from the file data. A typical 
Lustre cluster can have tens of thousands of clients, thousands of object 
storage servers (OSDs) and a failover pair of metadata servers (currently 
still a work in progress, metadata servers will in the future be able to 
form a cluster comprising of dozens of nodes). Lustre assumes that OSDs 
are reliable, i.e., use such techniques as RAID to prevent data loss.

\subsubsection{Lustre Architecture}
Lustre has a somewhat unique architecture, comprised of three major 
functional units. One is a single metadata server or MDS that contains 
a single metadata target or MDT for each Lustre filesystem. This stores 
namespace metadata, which includes filenames, directories, access 
permissions and file layout. The MDT data is stored in a single disk 
filesystem mapped locally to the serving node and is a dedicated 
filesystem that controls file access and informs the client node(s) 
which object(s) make up a file. Second are one or more object storage 
servers (OSSes) that store file data on one or more object storage 
targets or OST. An OST is a dedicated object-base filesystem exported 
for read/write operations. The capacity of a Lustre filesystem is 
determined by the sum of the total capacities of the OSTs. Finally, 
there's the client(s) that accesses and uses the file data. A diagram 
which depicts the Lustre high level architecture is presented below.

% diagram on Lustre architecture here



Lustre presents all clients with a unified namespace for all of the 
files and data in the filesystem that allow concurrent and coherent 
read and write access to the files in the filesystem. When a client 
accesses a file, it completes a filename lookup on the MDS, and 
either a new file is created or the layout of an existing file is 
returned to the client. Locking the file on the OST, the client will 
then run one or more read or write operations to the file but will 
not directly modify the objects on the OST. Instead, it will delegate 
tasks to the OSS. This approach will ensure scalability and improved 
security and reliability, as it does not allow direct access to the 
underlying storage, thus, increasing the risk of file system corruption 
from misbehaving/defective clients. Although all three components 
(MDT, OST and client) can run on the same node, they typically are 
configured on separate nodes communicating over a network.

Servers can currently be added dynamically and the file system is 
POSIX compliant. Other features that Lustre provides are ADIO 
interfaces, it can disable locking, and perform direct I/O that is 
usable for databases. Lustre also has other tuneable settings. It can 
currently be installed on Linux where it can interoperate amongst all 
supported processor architectures. It is still being developed for 
other operating systems.

The main advantage of Lustre is that it has very high parallel 
performance, it also has good file I/O and can handle requests for 
thousands of files. Lustre does not seem to be deployed frequently 
in clusters that stretch over large distances.

The chart below provides a feature comparison of the three systems.

\begin{center}
    \begin{tabular}{ | l | p{3cm} | p{3cm} | p{3cm} |}
    \hline
    Attribute & Tahoe & Ceph & Lustre \\ \hline
    License & GNU GPL & GNU LGPL & GNU GPL \\ \hline
    Data Primitive & object (file) & object (file) &
    object (file) \\ \hline\
    Data Placement & random & placement groups, pseudo-random mapping &   
    based on round robin and free space heuristics \\ \hline
    Metadata Handling & flat & multiple metadata servers & 
    max of two  metadata servers  \\ \hline
    Storage Tiers & none & through CRUSH rules & pools of object storage
    targets \\ \hline
    Failure Handling & assuming unreliable nodes & assuming unreliable
    nodes & assuming reliable nodes \\ \hline
    Replication & server side & server side & server side \\ \hline
    WAN Deployment & numerous & no known deployment & TeraGrid (scientific
    data) \\ \hline
    Client Interfacing &  The API exposes standard GET, PUT, POST, and
    DELETE methods, supports JSON and HTML output & native client file
    system, FUSE & native client file system, FUSE, clients may export
    NFS, CIFS \\ \hline
    Node Types & client/server, an introducer, a key generator & client,
    metadata, object & client, metadata, object \\ \hline
    \end{tabular}
\end{center}

While the features contrasted above provide a view across markedly different 
architectures and storage system applications in practice, they represent a 
robust mixture of recent, but both proven and experimental distributed file 
storage systems. As such, they illustrate many of the trade-offs we intend 
to explore in our evaluation testing.


\section{Implementation}

\subsection{Environment Setup}

%This section is for how the computers were initially configured/setup/etc

\subsection{Tahoe - Installation and Configuratio n}
%This section discusses installing/configuring Tahoe
Using our environment setup of three computers all running Ubuntu 11.10 with a partition mounted for use by the Tahoe Filesystem at /Tahoe we could begin to setup the Tahoe Least Authority File System. 

\begin{enumerate}
\item The first step for deploying Tahoe was to acquire the latest version of the Tahoe LAFS from their website for each of the computers. \cite{tahoe-lafs.org} At the time this was version 1.9.1. Once we had downloaded the allmydata-tahoe-1.9.1.zip file from the Tahoe LAFS website, we could move onto the next step.
\item The second step was to insure we had a compatible version of python installed on all the computers. \cite{python.org} For the installation of the Tahoe LAFS version 2.4.4 or greater is required. And Python v3 or greater would not work either. Python 2.7.2 was already installed on Ubuntu 11.10 so we could move onto the next step.
\item We now extracted the Tahoe LAFS source files from the allmydata-tahoe-1.9.1.zip file we acquired from their website. This resulted in a folder in our home directory, which is where we downloaded the file on each computer, called allmydata-tahoe-1.9.1. 
\item Now that we had the Tahoe LAFS source extracted we could build the tahoe executable using the following command inside the allmydata-tahoe-1.9.1 folder to build the source via the python setup file.
\subitem python setup.py build
\item After the build had finished we got a subdirectory called bin inside the source directory that contains the tahoe executable that is used to initialize and start, run, and stop the tahoe introducer and clients. We now repeated this build process on the remaining computers.
\item Now that we have the tahoe executable created on all the computers we needed to decide which computer was best suited to be the introducer as it will have a slightly heavier load as it will act as the node the clients connect to and due to our limited number of computers we will also have a client node running on it as well. We chose to use computer XPC1 for this due to the observational behavior of it was significantly more responsive than the other two computers, eventually a performance benchmark will be run on each of the computers to determine that this was the correct choice. As we wanted to get something up and running we went ahead on this assumption to see if we could get the Tahoe LAFS working on these computers.
\item The next step was to create subdirectories on XPC1 for the introducer and client to use as each will have their own configuration files. To this end an introducer and client directory were created in the /Tahoe directory. The creation of a client directory could be done on the other two computers could be done as well to make the commands used to get the Tahoe clients running consistent across all three computers; however, at this time this was not done. 
\item The next step was to get our introducer node created as we need it running before we can properly setup the client nodes. To create the introducer node we used the following command from the ~/allmydata-tahoe-1.9.1/bin directory where the tahoe executable lives:
\subitem tahoe create-introducer --node-directory=/tahoe/introducer

This creates the introducer configuration files inside the /tahoe/introducer folder and will allow us to edit the tahoe.cfg file with the configuration tweaks we need to make for our small number of available nodes. 
\item The next step is to edit the /tahoe/introducer/tahoe.cfg file to include the following changes:
\subitem nickname = xpc1
\subitem web.port = tcp:3457:interface=127.0.0.1
\subitem shares.happy = 2

These changes give the introducer a nickname that we can recognize and also provides us with a web interface port that is different from the default port for tahoe interfaces of 3456 so that eventually we will be able to see both the introducer and client's web interface for XPC1 with out them conflicting on the same port.

\item After we had made the necessitated changes to the introducer's tahoe.cfg file we saved it and could start the introducer running. This is necessary for the clients to be able to connect to the introducer, but also because it needs to be started at least once before the introducer.furl file is created. This is needed in the client configuration as this is how the clients find the introducer and function as a group. To start the introducer running we use the following command assuming we have added the tahoe executable to the path:
\subitem tahoe start --node-directory=/tahoe/introducer/

\item Now that we have the introducer running on XPC1 we are ready to create the client nodes. Instead of creating clients, we opted to just create nodes that share their storage vs only replicate their files across the available nodes as we only have three working computers. To create a node we used the following command on XPC2 \& XPC3:
\subitem tahoe create-node --node-directory=/tahoe/

And we used this command on XPC1:

\subitem tahoe create-node --node-directory=/tahoe/client/

This created all the configuration files for the client nodes in their respective directories. 

\item Now that we have the client directories created we want to open up the /tahoe/introducer/introducer.furl file and copy out the enclosed furl in the file so that we can put it in the client tahoe.cfg files. For our introducer we have the following contents for the introducer.furl file:
\subitem pb://6n56ctw77txdhrc2455jnwe3zzouho6n@192.168.1.10:57307,127.0.0.1:57307/introducer

\item We now open up the clients tahoe.cfg file and edit it to reflect the following changes, with what is in the [ ] indicating a value that varied between computers or is too long to fit below:
\subitem nickname = [name of computer]
\subitem introducer.furl = [contents of introducer.furl]
\subitem shares.happy = 2

With these changes our clients are ready to be started and have been configured to be able to work on our extremely small grid. 

\item The final step is to start the client nodes. We are going to use the command to run the tahoe client nodes in the background; however, it is worth mentioning that there is a command that would run it in the foreground in a UNIX environment instead that was used during our initial test run to see if the Tahoe LAFS was working. To start the clients on XPC2 \& XPC3 the following command was used:
\subitem tahoe start /tahoe/

And on XPC1 the following command was used:

\subitem tahoe start /tahoe/client/

This started the client nodes running in the background and our Tahoe LAFS grid is now installed, configured, and running.
\end{enumerate}

Now that we had the Tahoe LAFS running we verified that it was working by navigating to 127.0.0.1:3457 on XPC1 to verify that the introducer was working and check on its status. It showed us that it was running and that the three client nodes were connected. We then went to the web interface for the client node on XPC1 located at 127.0.0.1:3456 that provided us with an interface to upload \& download files, create directories, and showed us the status of the available storage nodes. We then uploaded an image file to verify that we could upload a file and then download it again. We were able to successfully retrieve the image file back out of the Tahoe LAFS. This was done to insure that we had in fact installed, configured, and gotten the Tahoe LAFS working. Further testing will happen in another section later on. 

\subsection{Ceph - Installation and Configuration}
Most Ceph installation guides recommend that the storage daemons in Ceph 
use btrfs for storing data and taking advantage of btrfs' internal 
transactions to keep the local data set in a consistent state. The claim 
is that this makes the storage cluster simple to deploy. In addition it 
is intended to provide scalability not currently available from block-
based Linux cluster file systems. Ceph provides some new features for 
Linux-based users. Directory snapshots provide users with the means to 
create read-only snapshots of directories and their nested contents using 
the command 'mkdir .snap/my_snapshot'.  Deletion is also a simple task 
using the command 'rmdir .snap/old_snapshot'.  Ceph also provides 
statistics on the number of nested files, directories, and file sizes 
for each directory. This makes it fairly easy to manage usage in the 
system.

Ceph uses a single configuration file to define cluster membership, 
hostnames, paths to devices, and runtime options. The configuration 
file is designed so that everything can be placed in a single file and 
shared unmodified on all hosts in the cluster. Hence, no per-node 
configuration file changes should be necessary. The default location 
for the configuration file is /etc/ceph/ceph.conf. A single master 
ceph.conf file that contains configuration information for the entire 
cluster can be maintained, and then configured by  a 'fetch_config' 
script on each node that will pull down the configuration when it is 
needed. For command line user tools (e.g., 'ceph'), you can define an 
/etc/ceph/ceph.conf that contains at a minimum the monitor ip addresses, 
as that is usually all you need. A different configuration file can be 
used on each host, defining parameters only for the daemons running on 
that host, but this is more difficult to maintain. The one point is that 
the configuration file on every host should include the monitor daemon 
sections and 'mon addr' option, so that the daemon knows how to join the 
cluster. Other monitor options can be omitted, except, of course, on the 
hosts running the monitors themselves.

Some other general configuration tips include: (1) ensure that the log 
partition is on a fast disk as Ceph produces a large quantity of logs, 
and, (2) enable noatime everywhere, particularly on the OSD store.
In creating a new file system, the --mkbtrfs option will create a new 
btrfs file system for each OSD and mount it for you. It requires that 
the 'btrfs devs' and 'btrfs are specified. Since no safety checks are 
made, one needs to be careful with the 'btrfs devs' option. To use ext4 
instead of btrfs, comment out "btrfs devs" in ceph.conf, point "osd data" 
to an already mounted ext4 partition and use mkcephfs without --mkbtrfs. 
The ext4 partition must be mounted with -o user_xattr or else mkcephfs 
will fail. Also using noatime, nodiratime boosts performance at no cost. 
When using ext4, you should disable the ext4 journal, because Ceph writes 
its own journals. This step can enhance performance.  Creating a new file 
system on an ext4 partition that already contains data, will invoke rm -rf 
to delete the data. If there is a lot of it, it might seem as if mkcephfs 
is hanging when it actually is not. 
The -k admin.keyring option lets you specify where mkcephfs puts your master 
admin key file. This is needed to administer the cluster when authentication 
is enabled. Various keys are generated even if authentication is disabled, 
so that it can be enabled later. 

Ssh keys need to be setup so that the machine running mkcephfs  (the master) 
can ssh in to other nodes  as root. The usual way to do this, assuming  no 
authorized_keys are set up, is to generated the public/private key pair and 
then ssh it to other slaves as the root. Once the file system is created and 
the Ceph daemon is running, it must be mounted. This done using  the kernel 
driver or with fuse. The monitor is always mounted. From there, getting a 
simple test cluster to start up out of your source directory is relatively 
straightforward.

Like most distributed file systems, Ceph doesn't deal well with OSDs that 
run out of space. To prevent even-more disastrous scenarios from occurring, 
once an OSD reaches pre-defined thresholds of disk usage the monitor will 
set a full flag on the OSDMap. Once that flag is set’ the OSDs will reject  
all client writes and all clients will pause writes. There are a few things 
that can be done to improve things in this situation:  (1) Rebalance the data 
by reweighting the OSDs, and (2) Increase the thresholds for the full flag.
Since the Ceph MDSes are essentially a single point of failure, there is a 
need to ensure that they recover quickly. To do this, Ceph supports the 
standby-replay mode. Herein, an MDS in standby-replay will continuously 
replay the journal of an active MDS. This does the following two things: 
(1) ensures that the journal is tested. If there are problems with it, the 
MDS will error out and a bug can be filed! (2) if the active MDS fails, the 
standby-replay takes over. Since it has already cached the appropriate 
metadata, it doesn't need to replay the entire journal.

Ceph is a complex project, with many independent components. Different parts 
of the system are tested differently.  At the lowest level, functional and 
minimal units of code are tested with unit tests. Unit tests are isolated 
from the real world in the sense that they do not use networking, special 
kernel features or such. This helps keep them fast, safe and easy to run.  
For simple command-line tools doing basic file manipulation, unit tests are 
difficult to write and integration tests represent test overkill. This is 
where cli tests come handy. The idea is to write the command to run, the 
expected output, and the expected exit code in a file. Organize tests based 
on the command line tool they are (primarily) testing. The cli tests can be 
run "./src/test/run-cli-tests", or just "make check".  Also check out the 
interactive mode of cram, that can helps you update tests when there are 
trivial changes -- do take care to read carefully still, and not just 
blindly accept the new output.  Additionally the tester should note that 
interactive mode can lose carefully crafted regexps. In more complex cases, 
testing may require regular expressions. This is done by adding "(re)" at 
the end of the line.

\subsection{Lustre - Installation and Configuration}
%section on implementing Lustre
In building Lustre from source, one needs to ensure that they are using 
Linux kernel 2.6.16 or greater. In all deployments of Lustre, the server 
that runs on an MDS, MGS or OSS needs to utilize a patched kernel. 
Running a patched kernel on a Lustre client is optional and required only 
if the client will be used for multiple purposes, such as running as both 
a client and an OST. The /boot/grub/grub.conf file needs to be reviewed 
to validate that the newly installed kernel has been set as the default 
kernel. Once all packages have been installed, a reboot is required to 
load the new kernel image. Once the system has been rebooted, an invocation 
of the uname command will reveal the currently booted kernel image. With 
respect to the client side, the packages for the lustre client and lustre 
client modules need to be installed on all desired client machines. Client 
machines need to be within the same network as the host machine serving the 
Lustre filesystem. After the packages are installed, a reboot must be 
performed on all affected client machines.

To configure the Lustre filesystem, one needs to configure Lustre Networking, 
or LNET, which provides the communication infrastructure required by the 
Lustre filesystem. LNET supports many commonly used network types, which 
include IP  networks. It allows simultaneous availability across different 
network types with routing between them. Note the role of the Management 
Server or MGS. The MGS stores configuration information for all Lustre 
filesystems in a clustered setup. An OST contacts the MGS to provide 
information, while the client(s) contact the MGS to retrieve information. 
The MGS requires its own disk for storage, but a provision exists to allow 
the MGS to share a disk with a single MDT. If nothing is provided then the 
default fsname is lustre. If one or more of these filesystems are created, 
it is necessary to use unique names for each labeled volume. These names 
become   important for when one accesses the target on the client system.
The OST is created by executing a set command. When the target needs to 
provide information to the MGS or when the client  accesses the target for 
information lookup, one needs to be aware of where the MGS is, which is 
defined for this target as the server's IP address followed by the network 
interface for communication. At this point one can mount newly formatted 
devices local to the host machine using a generic mount command. When one 
mounts the volume over the network on the client node, one must 
specify this name. This is done by following the network method by which 
the remote volume is mounted. Once mounted, the file system can be accessed 
by the client node. What that implies is that one cannot read or write from 
or to files located on the mounted OST. 

What is described above is a simple description of installing and 
configuring the Lustre distributed filesystem;  a more full accounting 
is beyond the scope of this paper. Suffice to say that much richer 
implementations are possible. As one example, Lustre can be configured 
for high availability to ensure that in the situation of failure, the 
system's services continue without interruption. That is, the 
accessibility between the client(s), server(s) and external target 
storage is always made available through a process called failover. 
Such high availability is provided through an implementation of disk 
drive redundancy in some sort of RAID configuration for the event of 
drive failures. In short, Lustre isn't a difficult technology to work 
with; however, there is an entire community with excellent administrator 
and developer resources ranging from articles, mailing lists and more 
to aid one in installation and maintenance issues that may arise. Also, 
commercial support for Lustre is provided by a large number of vendors 
who sell bundled computing and Lustre storage systems. Many of these 
vendors also contribute to the open source community's Lustre Project. 







\section{Evaluation}
From our review of the research literature we noted the following 
design characteristics with respect to storage that will be factored 
into our storage system implementation:

\begin{enumerate}
\item  System Function - DSS functionality displays a wide array 
of behavior, often well beyond typical store and retrieve.
\item Storage Architecture: We examined various architectures
employed by DSSs. Our investigation shows an evolution from 
centralized to the more recently favored decentralized approach.
\item Operating Environment: We became aware of how various
categories of operating environments influence design and
architecture.
\item Usage Patterns: We were made aware of classification of
various workloads experienced by DSSs and how the operating 
environment has a major influence on usage patterns.
\item  Consistency: Distributing, replicating and supporting
concurrent access are factors which challenge consistency. We
took note of various approaches used to enforce consistency
and the respective trade-offs in performance, availability and
choice of architecture.
\item Security: With attention turning towards applications
operating on the Internet, establishing a secure system is a
challenging task which is made increasingly more difficult as
DSSs adopt decentralized architectures. Our investigation
covered traditional means as well as more recent approaches
that have been developed for enforcing security in
decentralized architectures.
\item Automated Management: Systems are increasing in
complexity at an unsustainable rate. Features and services
need to address this problem by automating and abstracting
away system complexity, simplifying maintenance and
administration.
\item Federation: Many different formats and protocols are now
used to store and access data, creating a difficult environment
for easily sharing data and resources. Federation middleware is
needed to provide a single uniform homogeneous interface to
what would otherwise be a myriad of heterogeneous interfaces
and protocols. Federation enables the sharing of services,
while attempting to reduce effort wasted on duplication.
\end{enumerate}

By incorporating these observations into our design and any
services that we develop for implementation, we believe our
system will better address and reflect current and evolving
storage needs and provide insights into the tradeoffs and 
complexity involved in implementing today's storage technologies.

Going forward our plan is to build a parallel distributed file 
system implementation. Currently, we are considering using 
Tahoe and Ceph for this purpose. Once each system is in place, 
we will perform a performance and feature-set comparison between 
the two systems. This will allow us to create a report comparing 
the two systems and suggesting where each might be used most 
effectively.

As time allows, we would then like to build an application atop one
of these systems to demonstrate how one might take advantage of the
features each system provides. Alternatively, we may also choose to
modify one of these systems to extend its feature set or increase
its performance. Possible modifications may include increased 
security, increased ease of use, and/or increased robustness.
A third option might be to add additional file system implementations 
to our deployment in order to allow us to generate further comparisons 
and metrics.

Our immediate concern in meeting the schedule and requirements stated 
above is to secure the necessary hardware and resources for our 
testing platform. Ideally, we need a small collection of identical 
machines in order to facilitate neutral comparisons. We may also be 
able to utilize virtualized hardware. We are still exploring the options 
available to us.

\nocite{*}
\bibliography{refs}

\end{document}
