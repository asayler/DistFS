% Bryan Dixon
% Brendan Kelly
% Mark Lewis-Prazen
% Andy Sayler
% University of Colorado
% Distributed Systems
% Spring 2012

\documentclass[11pt]{article}

\usepackage[text={6.5in, 9in}, centering]{geometry}
\usepackage{graphicx}
\usepackage{url}
\usepackage{listings}
\usepackage{hyperref}

\bibliographystyle{plain}

\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\newenvironment{packed_enum}{
\begin{enumerate}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{enumerate}}

\newenvironment{packed_item}{
\begin{itemize}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}}

\begin{document}

\title{
  Distributed File Storage Systems\\
  Project Report 1
}
\author{
  Bryan Dixon \and Brendan Kelly \and Mark Lewis-Prazen \and Andy Sayler\\
  \and University of Colorado\\
  \texttt{first.last@colorado.edu}
}
\date{\today}

\maketitle

\begin{abstract}
The new generation of applications requires the processing of
terabytes and petabytes of data. This processing has largely been
achieved by distributed processing architectures and methodologies,
driving the rise of companies such as Google, Amazon and Yahoo, which
have embraced such technologies over the past decade and made them a
central part of their business models. The increased importance of
large scale data processing and a corresponding explosion in
alternative storage architectures and service strategies requires
computer science researchers and professionals to become well versed
in the available alternatives and trade-offs in making technological
architectural choices these areas. While storage technology has
evolved significantly over the past five years, a deep understanding
of both technical and business storage issues has lagged behind as
many in these fields still view data storage as a mundane activity
still the purview of back office technocrats. The current trend in
distributed cloud storage and the focus of delivering IT
infrastructure and applications bundled with data storage has led to
an “as a service” model being used to promote such products.

The intent of our project is to examine the current state of both core
storage and related service offerings on both a technical and business
level to understand (1) what is technically available, (2) what are
current research and developmental directions in storage technology,
and (3) how core storage architectures are being coupled with
associated services to produce bundled storage product offerings that
purport to add value above and beyond core offerings. We believe that
by performing this analysis and the subsequent implementation of a
distributed proof of concept storage model, we will gain a better
understanding of evolving storage technologies as well as the
tradeoffs that need to be made when implementing bundled storage
services in a production environment.

\end{abstract}

\newpage

\section{Problem Definition}
Along with the infrastructure and network based applications, data 
storage is recognized as one of the major components of information 
technology systems.  Functionally, storage typically services a wide 
range of requirements, spanning the spectrum from caching to archival 
needs. Combining networking and storage has created a platform with 
numerous possibilities allowing Distributed Storage Systems (DSS) to 
adopt roles which can vary considerably and may fall well beyond 
traditional data storage needs. Most DSSs as we understand them today 
were in their inception known as Distributed File Systems (DFS). Networking 
infrastructure and distributed computing share a close relationship. 
Advances in networking are typically closely followed by new distributed 
storage systems, which are designed to better utilize the enhanced 
capabilities of the network. Currently, the next generation Internet based 
systems are encountering numerous challenges, among them longer delays, 
unreliability, unpredictability and the potential for malicious behavior, 
all because of the need to operate in a public shared environment. To address 
this new set of challenges, innovative new storage architectures and 
algorithms are being suggested and developed, offering a myriad of 
improvements in areas such as security, consistency and routing. 
As storage systems continue to evolve, they naturally increase in complexity. 
Hence, the requisite expertise required to implement and operate them also 
increases dramatically. 

Industry storage practices have evolved considerably over the past decade. The 
amount of data being actively managed continues to expand at around 20% per year, 
with many firms dealing with growth rates exceeding 50%. At these levels, most 
data centers will double storage capacity every two to three years. Meanwhile, 
IT budgets are generally in decline. Why isn’t industry-wide chaos rampant? In 
a word, this is due to consolidation efforts. Consolidation is being aided by 
continued evolution of high-density magnetic media as well as bigger, faster, and 
less expensive solid-state drives. Virtualization is also helping by easing 
management of aggregated storage pools that are using available capacity more 
efficiently. Optimization technologies like data reduction, thin provisioning, 
and automatic tiering are also providing significant benefits.

What has become increasingly apparent in production environments is that the 
storage decision is no longer relegated to a back office bureaucracy which 
manages an organization’s storage needs en masse. Rather, as organizations 
increasingly decentralize and decouple decision-making regarding many applications 
either in the development or maintenance cycle, storage decisions are being made 
on a more ad hoc basis. Also, as more organizations consider or adopt partial or 
full cloud or virtualization storage strategies, storage decisions have become 
more application-centric. Since full migrations to the cloud or to virtual storage 
models are generally infeasible in the short term, except in the case of smaller 
organizations, more firms are increasingly managing a tiered storage model. Recent 
published research by International Data Corporation (IDC) and others demonstrate 
that storage utilization rates achieved by most U.S. companies are typically 40% 
or lower. Hence there appears to be considerable room for improvement in both 
storage management and implementation practices across the industry. 

The above noted disruptive forces in the storage industry are creating a unique 
set of opportunities and challenges. New firms are appearing that offer storage 
solutions and services bundled in ways which were virtually unheard of five to 
seven years ago. At the same time organizations and IT managements under increasing 
pressure to reduce storage costs, increase storage utilization rates and provide 
both a secure and transparent accessibility to their data users regardless of 
the ultimate location of a particular data set. Our project is focussed on providing 
methods and techniques to allow the computer science professionalto better understand 
and navigate this new and complex storage landscape and to discover the choices and 
tradeoffs that one must consider in designing and implementing a sensible organization-
wide storage solution.

\section{Introduction}


\section{Related Work}
\subsection{Tahoe File System}

The Tahoe-LAFS (Tahoe Least-Authority Filesystem) is a secure,
distributed filesystem designed by Zooko Wolcox-O'Hearn and Brain
Warner of allmydata.com. Tahoe-LAFS (henceforth referred to as
'Tahoe') is designed around the Principle of Least Authority, and aims
to allow one to deploy a trusted distributed filesystem using
untrusted, or even actively malicious, nodes. Tahoe is also designed
to be fault-tolerant, and to run on commodity hardware where failures
may occur frequently. Tahoe uses Reed-Solomon erasure coding to obtain
redundancy across nodes. It uses convergent encryption in conjunction
with a capability-based access control model, to obtain and maintain
security \cite{WilcoxOHearn:2008p1275}.

Tahoe presents a web-API to users of the system that can be used to
administrate the system, as well as to read, write, or verify files
and directories on the system. This web-based API makes Tahoe suitable
for use as the backend for various services and applications in
domains such as backup and cloud storage. Tahoe is still actively
maintained and is Free Software under the GPL and TGPL. It served as the
backend for allmydata.com until allmydata.com went out of business. It
is currently deployed for personal use be a number of individuals, and
serves as the storage backend for several other
systems \cite{tahoe-lafs.org}. 

\subsection{Ceph}
This section will be about the Ceph File System
\cite{Weil:2012p1035,Weil:2012p1010,Weil:2006p1273}.

\nocite{*}
\bibliography{refs}

\end{document}
